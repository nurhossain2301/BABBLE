audio_enc_dim1: 768
audio_enc_dim2: 1280
llm_dim: 2048
audio_encoder_name: null 
audio_model_path: null
linear_neurons: 1
input_size: 12
connector_name: "linear-pool"
llm_name:  "TinyLlama/TinyLlama-1.1B-Chat-v1.0" ##[ "meta-llama/Llama-3.2-3B-Instruct" "TinyLlama/TinyLlama-1.1B-Chat-v1.0" "meta-llama/Llama-3.1-8B-Instruct" ]  
llm_model_path: null 
finetune_encoder: False
freeze_linear_layer: False
audio_length: 30
target_length: 0.1 
connector_k: 10
connector_k2: 10
use_lora: True
lora_r: 16
lora_alpha: 32
max_lr: 0.0001
warmup_steps: 100
grad_accumulate_steps: 2
apply_suggest_word: "combined" # choose from ["combined","separate", None], None for greedy decoding

#"google/gemma-3-4b-pt"   #"meta-llama/Llama-3.1-8B-Instruct" #"TinyLlama/TinyLlama-1.1B-Chat-v1.0"    #mistralai/Mixtral-8x7B-v0.1